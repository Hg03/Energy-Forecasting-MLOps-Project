# Data Validation for Quality and Integrity using GE. Model Performance Continuous Monitoring

This tutorial represents **lesson 5 out of a 7-lesson course** that will walk you step-by-step through how to **design, implement, and deploy an ML system** using **MLOps good practices**. During the course, you will build a production-ready model to forecast energy consumption levels for the next 24 hours across multiple consumer types from Denmark.

By the end of this course, you will understand all the fundamentals of designing, coding and deploying an ML system using a batch-serving architecture.

This course targets mid/advanced machine learning engineers who want to level up their skills by building their own end-to-end projects.

## Table of Contents

- Course Introduction
- Course Lessons
- Data Source
- Lesson 5: Data Validation for Quality and Integrity using GE. Model Performance Continuous Monitoring.
- Lesson 5: Code
- Conclusion
- References

## Course Introduction

**At the end of this 7 lessons course, you will know how to:**

- design a batch-serving architecture
- use Hopsworks as a feature store
- design a feature engineering pipeline that reads data from an API
- build a training pipeline with hyper-parameter tunning
- use W&B as an ML Platform to track your experiments, models, and metadata
- implement a batch prediction pipeline
- use Poetry to build your own Python packages
- deploy your own private PyPi server
- orchestrate everything with Airflow
- use the predictions to code a web app using FastAPI and Streamlit
- use Docker to containerize your code
- use Great Expectations to ensure data validation and integrity
- monitor the performance of the predictions over time
- deploy everything to GCP
- build a CI/CD pipeline using GitHub Actions

If that sounds like a lot, don't worry. After you cover this course, you will understand everything I said before. Most importantly, you will know WHY I used all these tools and how they work together as a system.

By the end of the course, you will know how to implement the diagram below. Don't worry if something doesn't make sense to you. I will explain everything in detail.

![image](https://github.com/Hg03/mlops-paul/assets/69637720/bc9ee26a-09db-411b-831f-dabc0a5c818f)

By the end of Lesson 5, you will know how to use Great Expectations to validate the integrity and quality of your data. Also, you will understand how to implement a monitoring component on top of your ML system.


## Course Lessons:

1. [Batch Serving. Feature Stores. Feature Engineering Pipelines.](https://github.com/Hg03/mlops-paul/blob/main/Lesson_1.md)
2. [Training Pipelines. ML Platforms. Hyperparameter Tuning.](https://github.com/Hg03/mlops-paul/blob/main/Lesson_2.md)
3. [Batch Prediction Pipeline. Package Python Modules with Poetry.](https://github.com/Hg03/mlops-paul/blob/main/Lesson_3.md)
4. [Private PyPi Server. Orchestrate Everything with Airflow.](https://github.com/Hg03/mlops-paul/blob/main/Lesson_4.md)
5. **Data Validation for Quality and Integrity using GE. Model Performance Continuous Monitoring.**
6. [consume and Visualize your Modelâ€™s Predictions using FastAPI and Streamlit. Dockerize Everything.](https://github.com/Hg03/mlops-paul/blob/main/Lesson_6.md)
7. [Deploy All the ML Components to GCP. Build a CI/CD Pipeline Using Github Actions.](https://github.com/Hg03/mlops-paul/blob/main/Lesson_7.md)
8. [Bonus - Behind the Scenes of an â€˜Imperfectâ€™ ML Project â€” Lessons and Insights](https://github.com/Hg03/mlops-paul/blob/main/Bonus.md)

## The goal of lesson 5

At this point, the ML pipeline is implemented and orchestrated. That means we are done, right?

Not quiteâ€¦

One final step, which will transform you from a good engineer to an excellent one, is adding a component that will allow you to quickly diagnose what is happening in your production system.

During Lesson 5, you will primarily learn 2 different topics that serve one single goal: to ensure your production system is working correctly.

1. **Data Validation:** check if the data generated by the FE pipeline is OK before ingesting it into the Feature Store.

2. **Model Monitoring:** continually compute various metrics that reflect the performance of your production model.

![image](https://github.com/Hg03/mlops-paul/assets/69637720/98a345a8-efb7-4f34-8924-5363f213e5f1)

I will go into more detail in the Theoretical Concepts & Tools section. Still, as a brief overview, to continually monitor your model's performance, you will use your old predictions with the newly gathered ground truth to compute a desired metric, in your case MAPE.

For example, you predict the energy consumption values from the 1st of June for 24 hours. Initially, you don't have the data to compute the metrics. But, after 12 hours, you can collect the real energy consumption. Thus, you just put your hands on the ground truth to compute the desired metrics for the last 12 hours.

After 1 hour, you can compute the metric for another data point, and so onâ€¦

This is the strategy we will adopt in this tutorial.

## Theoretical Concepts & Tools

**Data Validation:** Data validation refers to the process of ensuring data quality and integrity. What do I mean by that?

As you automatically gather data from different sources (in our case, an API), you need a way to continually validate that the data you just extracted follows a set of rules that your system expects.

For example, you expect that the energy consumption values are:

- of type float,
- not null,
- â‰¥0.

While you developed the ML pipeline, the API returned only values that respected these terms, as data people call it: a "data contract."

But, as you leave your system to run in production for a 1 month, 1 year, 2 years, etc., you will never know what could change to data sources you don't have control over.

Thus, you need a way to constantly check these characteristics before ingesting the data into the Feature Store.

**Note:** To see how to extend this concept to unstructured data, such as images, you can check my Master [Data Integrity to Clean Your Computer Vision Datasets](https://medium.com/towards-data-science/master-data-integrity-to-clean-your-computer-vision-datasets-df432cf9e596) article.

Great Expectations (aka GE): GE is a popular tool that easily lets you do data validation and report the results. Hopsworks has GE support. You can add a GE validation suit to Hopsworks and choose how to behave when new data is inserted, and the validation step fails â€” [read more about GE + Hopsworks](https://www.hopsworks.ai/post/data-validation-for-enterprise-ai-using-great-expectations-with-hopsworks).

![image](https://github.com/Hg03/mlops-paul/assets/69637720/ad062a1f-7e85-46f2-b897-7f186d650180)

**Ground Truth Types:** While your model is running in production, you can have access to your ground truth in 3 different scenarios:

1. **real-time:** an ideal scenario where you can easily access your target. For example, when you recommend an ad and the consumer either clicks it or not.
2. **delayed:** eventually, you will access the ground truths. But, unfortunately, it will be too late to react in time adequately.
3. **none:** you can't automatically collect any GT. Usually, in these cases, you have to hire human annotators if you need any actuals.

![image](https://github.com/Hg03/mlops-paul/assets/69637720/9da963e6-9a29-4fdc-8629-8a7364b92ac3)

In our case, we are somewhere between #1. and #2. The GT isn't precisely in real-time, but it has a delay only of 1 hour.

Whether a delay of 1 hour is OK depends a lot on the business context, but let's say that, in your case, it is okay.

As we considered that a delay of 1 hour is ok for our use case, we are in good luck: we have access to the GT in real-time(ish).

This means we can use metrics such as MAPE to monitor the model's performance in real-time(ish).

In scenarios 2 or 3, we needed to use data & concept drifts as proxy metrics to compute performance signals in time.

![image](https://github.com/Hg03/mlops-paul/assets/69637720/07750300-e7a8-4c1b-ac05-86d07ffaeb6b)

**ML Monitoring:** ML monitoring is the process of assuring that your production system works well over time. Also, it gives you a mechanism to proactively adapt your system, such as retraining your model in time or adapting it to new changes in the environment.

In our case, we will continually compute the MAPE metric. Thus, if the error suddenly spikes, you can create an alarm to inform you or automatically trigger a hyper-optimization tuning step to adapt the model configuration to the new environment.

![image](https://github.com/Hg03/mlops-paul/assets/69637720/98d65399-7d2b-48de-959a-651f8ff52c8e)

## Lesson 5 : Code

The code within Lesson 5 is located under the following:

- [feature-pipeline](https://github.com/iusztinpaul/energy-forecasting/tree/main/feature-pipeline) folder â€” **data validation**,
- [batch-prediction-pipeline](https://github.com/iusztinpaul/energy-forecasting/tree/main/batch-prediction-pipeline) folder â€” **ML monitoring**.

Using Docker, you can quickly host everything inside Airflow, so you don't have to waste a lot of time setting things up.

Directly storing credentials in your git repository is a huge security risk. That is why you will inject sensitive information using a **.env** file.

The **.env.default** is an example of all the variables you must configure. It is also helpful to store default values for attributes that are not sensitive (e.g., project name).

![image](https://github.com/Hg03/mlops-paul/assets/69637720/dd31cc57-9c78-4a6a-ae1c-06c4d8b1da5c)

## Prepare credentials

I don't want to repeat myself too much. You already have step-by-step instructions on how to set up your credentials in the **"Prepare Credentials"** of previous lessons.

Fortunately, in this article, you don't have to prepare additional credentials from previous lessons.

Checking the **â€œPrepare Credentialsâ€** of Lesson 4 is a great starting point showing you how to prepare all your credentials and tools. Also, check the GitHub Repository for additional information.

It will show you how to complete all your credentials in your **.env** file.

Now, let's start coding ðŸ”¥

[Video](https://youtu.be/gLMEMxmuqsc)

The GE suit is defined in the [feature-pipeline/feature_pipeline/etc/validation.py](https://github.com/iusztinpaul/energy-forecasting/blob/main/feature-pipeline/feature_pipeline/etl/validation.py) file.

In the code below, you defined a GE **ExpectationSuite** called **energy_consumption_suite**.

Using the **ExpectationConfiguration** class, you can add various validation tests. In the following example, 2 tests were added:

1. Checks if the columns of the table match with a given ordered list.
2. Checks the length of the columns to be equal to 4.

Easy and powerful ðŸ”¥

```python
from great_expectations.core import ExpectationSuite, ExpectationConfiguration


def build_expectation_suite() -> ExpectationSuite:
    """
    Builder used to retrieve an instance of the validation expectation suite.
    """

    expectation_suite_energy_consumption = ExpectationSuite(
        expectation_suite_name="energy_consumption_suite"
    )

    # Columns.
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_table_columns_to_match_ordered_list",
            kwargs={
                "column_list": [
                    "datetime_utc",
                    "area",
                    "consumer_type",
                    "energy_consumption",
                ]
            },
        )
    )
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_table_column_count_to_equal", kwargs={"value": 4}
        )
    )
```

Now, let's take a look at the full validation suit ðŸ‘‡

Using GE, you will check a Pandas DataFrame for the following characteristics:

- The columns should be equal to: ["datetime_utc,"â€¦ "energy_consumption"].
- The DF should have exactly 4 columns.
- Column "datetime_utc" should have all the values different than null.
- Column "area" expects only values equal to 0, 1 or 2.
- Column "area" should be of type int8.
- Column "consumer_type" expects only values equal to 111, â€¦
- Column "consumer_type" should be of type int32.
- Column "energy_consumption" should have values â‰¥ 0.
- Column "energy_consumption" should be of type float64.
- Column "energy_consumption" should have all the values different than null.

```python
from great_expectations.core import ExpectationSuite, ExpectationConfiguration


def build_expectation_suite() -> ExpectationSuite:
    """
    Builder used to retrieve an instance of the validation expectation suite.
    """

    expectation_suite_energy_consumption = ExpectationSuite(
        expectation_suite_name="energy_consumption_suite"
    )

    # Columns.
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_table_columns_to_match_ordered_list",
            kwargs={
                "column_list": [
                    "datetime_utc",
                    "area",
                    "consumer_type",
                    "energy_consumption",
                ]
            },
        )
    )
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_table_column_count_to_equal", kwargs={"value": 4}
        )
    )

    # Datetime UTC
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_values_to_not_be_null",
            kwargs={"column": "datetime_utc"},
        )
    )

    # Area
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_distinct_values_to_be_in_set",
            kwargs={"column": "area", "value_set": (0, 1, 2)},
        )
    )
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_values_to_be_of_type",
            kwargs={"column": "area", "type_": "int8"},
        )
    )

    # Consumer type
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_distinct_values_to_be_in_set",
            kwargs={
                "column": "consumer_type",
                "value_set": (
                    111,
                    112,
                    119,
                    121,
                    122,
                    123,
                    130,
                    211,
                    212,
                    215,
                    220,
                    310,
                    320,
                    330,
                    340,
                    350,
                    360,
                    370,
                    381,
                    382,
                    390,
                    410,
                    421,
                    422,
                    431,
                    432,
                    433,
                    441,
                    442,
                    443,
                    444,
                    445,
                    446,
                    447,
                    450,
                    461,
                    462,
                    999,
                ),
            },
        )
    )
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_values_to_be_of_type",
            kwargs={"column": "consumer_type", "type_": "int32"},
        )
    )

    # Energy consumption
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_min_to_be_between",
            kwargs={
                "column": "energy_consumption",
                "min_value": 0,
                "strict_min": False,
            },
        )
    )
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_values_to_be_of_type",
            kwargs={"column": "energy_consumption", "type_": "float64"},
        )
    )
    expectation_suite_energy_consumption.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_values_to_not_be_null",
            kwargs={"column": "energy_consumption"},
        )
    )

    return expectation_suite_energy_consumption
```

**As you can see, the quality checks mostly resume to:**

1. Check the schema of the table.
2. Check the type of the columns.
3. Check the values of the columns (different logic for discrete or continuous features).
4. Check for nulls.

You will attach this validation suit in the **to_feature_store()** loading function of the FE pipeline from the [feature-pipeline/feature_pipeline/etl/load.py](https://github.com/iusztinpaul/energy-forecasting/blob/main/feature-pipeline/feature_pipeline/etl/load.py) file.

Now Hopsworks will run the given GE validation suit every time a new DataFrame is inserted into the feature group.

You can choose to reject the new data if the validation suit fails or to get an alarm to take manual action.

```python
import hopsworks
import pandas as pd
from great_expectations.core import ExpectationSuite
from hsfs.feature_group import FeatureGroup

from feature_pipeline.settings import SETTINGS


def to_feature_store(
    data: pd.DataFrame,
    validation_expectation_suite: ExpectationSuite,
    feature_group_version: int,
) -> FeatureGroup:
    """
    This function takes in a pandas DataFrame and a validation expectation suite,
    performs validation on the data using the suite, and then saves the data to a
    feature store in the feature store.
    """
    
    ...
    
    # Create feature group.
    energy_feature_group = feature_store.get_or_create_feature_group(
        name="energy_consumption_denmark",
        version=feature_group_version,
        description="Denmark hourly energy consumption data. Data is uploaded with an 15 days delay.",
        primary_key=["area", "consumer_type"],
        event_time="datetime_utc",
        online_enabled=False,
        expectation_suite=validation_expectation_suite,
    )
    # Upload data.
    energy_feature_group.insert(
        features=data,
        overwrite=False,
        write_options={
            "wait_for_job": True,
        },
    )
    
    ...
    
```
## ML Monitoring

[Video](https://youtu.be/uUF3iBj4Yyc)

When it comes to ML monitoring, the hardest part isn't the code itself but mostly choosing how to monitor your ML models.

Note that tools, such as [Evidently](https://www.evidentlyai.com/) or [Arize](https://arize.com/), are usually used for ML monitoring. But in this case, I wanted to keep it simple and not add another tool to the series.

But the concepts remain the same, which is the most crucial to understand.

In the code snippet below, we did the following:

    Loaded the predictions from the GCP bucket. All the predictions are aggregated in the predictions_monitoring.parquet file during the batch prediction step.
    Prepared the structure of the predictions DataFrame.
    Connected to the Hopsworks Feature Store.
    Queried the Feature Store for data within the minimum and maximum predictions datetime edges. This is your GT. You want to get everything available based on your prediction's datetime window.
    Prepared the structure of the GT DataFrame.
    Merge the two DataFrames.
    Where the GT is available, compute the MAPE metric. Out of simplicity, you will compute the MAPE metrics aggregated over all the time series.
    Write the results back to the GCP bucket, which will be loaded & displayed by the frontend.
